{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyMMY9kD+zm1VSIxeIwqXjFy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nnamaka/ML_specializations/blob/main/Computer_vision/face_Detection/faceRecognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MNJNem1DpR9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Install Dependencies"
      ],
      "metadata": {
        "id": "_xb9XHGpPQ9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.4.1 tensorflow-gpu==2.4.1 opencv-python matplotlib\n",
        "!pip install wget"
      ],
      "metadata": {
        "id": "Y9UMtA96PW24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Import Dependencies"
      ],
      "metadata": {
        "id": "XQlJCBB_PkQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "GCaHsD09PuKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import tensorflow dependencies - Functional API\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Layer, Conv2D, Dense, MaxPooling2D, Input, Flatten\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "7SwEiaa8QDZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Create Folder Structures\n"
      ],
      "metadata": {
        "id": "5Ht1qzKOVeN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "POS_PATH = os.path.join('data', 'positive')\n",
        "NEG_PATH = os.path.join('data', 'negative')\n",
        "ANC_PATH = os.path.join('data', 'anchor')"
      ],
      "metadata": {
        "id": "KDigIfCAFAl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make the directories\n",
        "os.makedirs(POS_PATH)\n",
        "os.makedirs(NEG_PATH)\n",
        "os.makedirs(ANC_PATH)"
      ],
      "metadata": {
        "id": "mdmQcQdiFW9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Collect Positive and Anchors.   \n",
        "And preprocess them"
      ],
      "metadata": {
        "id": "xLDpuAVAzk4c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  \n",
        "My positive and anchor images were stored in my Google Drive."
      ],
      "metadata": {
        "id": "ZB0WKwLPFpGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "u1usYtu6R7Pi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp \"/content/drive/MyDrive/TFOD images/face.tar.gz\" /content && cp \"/content/drive/MyDrive/TFOD images/anchor.tar.gz\" /content"
      ],
      "metadata": {
        "id": "Bu5ibPKfpDeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "uncompressed data file of positive image"
      ],
      "metadata": {
        "id": "kLov-E8FC9tU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xzf face.tar.gz && tar -xzf anchor.tar.gz"
      ],
      "metadata": {
        "id": "2ymx8putOjKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "resize and store positive and anchor images in data folder, in their appropriate folders."
      ],
      "metadata": {
        "id": "0Xfyod7byUqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "width = 250\n",
        "height = 250\n",
        "dim = (width, height)\n"
      ],
      "metadata": {
        "id": "dqSn38DPyc67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anch_pos = [\"/content/Anchor/\",\"/content/Myimages\"]\n",
        "\n",
        "for path in anch_pos:\n",
        "  for filename in os.listdir(path):\n",
        "    if filename.endswith('.jpg'):\n",
        "      try:\n",
        "        if 'Anchor' in path:\n",
        "          img = cv2.imread(\"/content/Anchor/\" + filename, cv2.IMREAD_UNCHANGED)\n",
        "          new_img = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
        "          cv2.imwrite(ANC_PATH + '/' + filename, new_img)\n",
        "\n",
        "        if 'Myimages' in path:\n",
        "          img = cv2.imread(\"/content/Myimages/\" + filename, cv2.IMREAD_UNCHANGED)\n",
        "          new_img = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
        "          cv2.imwrite(POS_PATH + '/' + filename, new_img)\n",
        "      except:\n",
        "        pass\n",
        "      "
      ],
      "metadata": {
        "id": "lSIYLsAZzvsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Augument all positive and anchor images to increase data size and possible expose the model to newer samples"
      ],
      "metadata": {
        "id": "6flaLa1gCn13"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def augment(img):\n",
        "  data = []\n",
        "  for i in range(15):\n",
        "    img = tf.image.stateless_random_brightness(img, max_delta=0.02, seed=(1,2))\n",
        "    img = tf.image.stateless_random_contrast(img, lower=0.6, upper=1, seed=(1,3))\n",
        "    img = tf.image.stateless_random_flip_left_right(img, seed=(np.random.randint(100), np.random.randint(100)))\n",
        "    img = tf.image.stateless_random_jpeg_quality(img, min_jpeg_quality=90, max_jpeg_quality=100, seed=(np.random.randint(100),np.random.randint(100)))\n",
        "    img = tf.image.stateless_random_saturation(img, lower=0.9,upper=1, seed=(np.random.randint(100),np.random.randint(100)))\n",
        "\n",
        "    data.append(img)\n",
        "\n",
        "  return data"
      ],
      "metadata": {
        "id": "r-_qRfliC5Bv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import uuid"
      ],
      "metadata": {
        "id": "dotL6qELLvOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paths = [POS_PATH, ANC_PATH]\n",
        "\n",
        "for path in paths:\n",
        "  print(\"inside \" + path)\n",
        "  for file_name in os.listdir(os.path.join(path)):\n",
        "    img_path = os.path.join(path, file_name)\n",
        "    img = cv2.imread(img_path)\n",
        "    augmented_images = augment(img) \n",
        "    \n",
        "    for image in augmented_images:\n",
        "        cv2.imwrite(os.path.join(path, '{}.jpg'.format(uuid.uuid1())), image.numpy())"
      ],
      "metadata": {
        "id": "KBBtvhO-mv39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "check the no of positive and anchor images"
      ],
      "metadata": {
        "id": "p8IPDpxwoIt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for path in paths:\n",
        "  print(\"no of images in \" + path + \" : \", end=\"\")\n",
        "  a = 0\n",
        "  for image in os.listdir(os.path.join(path)):\n",
        "    if image.endswith('.jpg'):\n",
        "      a = a + 1\n",
        "  print(a)"
      ],
      "metadata": {
        "id": "VBWG2lEUoTiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Collect Negative Images"
      ],
      "metadata": {
        "id": "ia4tUZTCQjl4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O wildImages.tar http://vis-www.cs.umass.edu/lfw/lfw.tgz"
      ],
      "metadata": {
        "id": "_jGoz1IXorPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Untar Labelled Faces in the wild Dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "P-9fwnZBF-MT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xzf wildImages.tar"
      ],
      "metadata": {
        "id": "6BxCyZaMmm5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# move ifw images to the negative folder image directory\n",
        "# a = 0\n",
        "# no_of_neg_img = 450\n",
        "for directory in os.listdir('lfw'):\n",
        "  for file in os.listdir(os.path.join('lfw', directory)):\n",
        "    # a = a + 1;\n",
        "\n",
        "    # if ( a == no_of_neg_img):\n",
        "    #   break\n",
        "\n",
        "    old_path = os.path.join('lfw', directory, file)\n",
        "    new_path = os.path.join(NEG_PATH, file)\n",
        "    os.replace(old_path, new_path)"
      ],
      "metadata": {
        "id": "IXbP-9PbnBwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load and Preprocess Images"
      ],
      "metadata": {
        "id": "-Y4lrPAUGNMQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Get image directories"
      ],
      "metadata": {
        "id": "mkR8nI7eFXeC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "anchor = tf.data.Dataset.list_files(ANC_PATH+'/*.jpg').take(450)\n",
        "positive = tf.data.Dataset.list_files(POS_PATH+'/*.jpg').take(450)\n",
        "negative = tf.data.Dataset.list_files(NEG_PATH+'/*.jpg').take(450)\n"
      ],
      "metadata": {
        "id": "9SjbqqOeFatT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dir_test = anchor.as_numpy_iterator()"
      ],
      "metadata": {
        "id": "BATz018fGkLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dir_test.next())"
      ],
      "metadata": {
        "id": "qZrkp049Gnaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocessing - Scale and Resize"
      ],
      "metadata": {
        "id": "ULIHARpMGshc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(file_Path):\n",
        "\n",
        "  byte_img = tf.io.read_file(file_Path)\n",
        "\n",
        "  img = tf.io.decode_jpeg(byte_img)\n",
        "\n",
        "  img = tf.image.resize(img, (100, 100))\n",
        "  img = img / 255.0\n",
        "\n",
        "  return img"
      ],
      "metadata": {
        "id": "oD-UZll2GxgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = preprocess('data/anchor/IMG_20220920_152033_954.jpg')\n"
      ],
      "metadata": {
        "id": "shtlEl0TKSwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img.numpy().max() \n"
      ],
      "metadata": {
        "id": "3tcQ1_2wKS7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create Labelled Dataset"
      ],
      "metadata": {
        "id": "Pw5VO30-KTzu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "positives = tf.data.Dataset.zip((anchor, positive, tf.data.Dataset.from_tensor_slices(tf.ones(len(anchor)))))\n",
        "negatives = tf.data.Dataset.zip((anchor, negative, tf.data.Dataset.from_tensor_slices(tf.zeros(len(anchor)))))\n",
        "data = positives.concatenate(negatives)\n"
      ],
      "metadata": {
        "id": "GpBWwVF1j36J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples = data.as_numpy_iterator()"
      ],
      "metadata": {
        "id": "InPZYs91kHsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = samples.next()"
      ],
      "metadata": {
        "id": "KWzOgaD_kLpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example"
      ],
      "metadata": {
        "id": "WD6Yvd0ykQT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Build Train and Test Partition"
      ],
      "metadata": {
        "id": "3efUieojkSxO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_twin(input_img, validation_img, label):\n",
        "  return( preprocess(input_img), preprocess(validation_img), label)"
      ],
      "metadata": {
        "id": "WMCLcJ6GkaCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = preprocess_twin(*example)"
      ],
      "metadata": {
        "id": "FNy6X7lVkqUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(res[1])"
      ],
      "metadata": {
        "id": "Kkr1dsyPkw9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res[2]"
      ],
      "metadata": {
        "id": "DddbyPGPl4HH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build dataloader and split data into train and test partition"
      ],
      "metadata": {
        "id": "Xtm95k4hl-VJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.map(preprocess_twin)\n",
        "data = data.cache()\n",
        "data = data.shuffle(buffer_size=1000)"
      ],
      "metadata": {
        "id": "7RQnYliwmCnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train partition\n",
        "train_data = data.take(round(len(data)*.7))\n",
        "train_data = train_data.batch(16)\n",
        "train_data = train_data.prefetch(8)"
      ],
      "metadata": {
        "id": "juTid7xMmfD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test partition\n",
        "test_data = data.skip(round(len(data)*.7))\n",
        "test_data = test_data.take(round(len(data)*.3))\n",
        "test_data = test_data.batch(16)\n",
        "test_data = test_data.prefetch(8)"
      ],
      "metadata": {
        "id": "_oC3aCcwm5Qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XuwUU6FMnOUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Engineering"
      ],
      "metadata": {
        "id": "GKjNe78SnVIH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Build Embedding Layer   \n",
        "create a function that creates our embedding layer"
      ],
      "metadata": {
        "id": "V-YiZ4rZnaWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_embedding():\n",
        "  inp = Input(shape=(100, 100,3), name='input_image')\n",
        "\n",
        "  # block 1\n",
        "  c1 = Conv2D(64, (10,10), activation='relu')(inp)\n",
        "  m1 = MaxPooling2D(64, (2,2), padding='same')(c1)\n",
        "\n",
        "  # block 2\n",
        "  c2 = Conv2D(128, (7,7), activation='relu')(m1)\n",
        "  m2 = MaxPooling2D(64, (2,2), padding='same')(c2)\n",
        "\n",
        "  # block 3\n",
        "  c3 = Conv2D(128, (4,4), activation='relu')(m2)\n",
        "  m3 = MaxPooling2D(64, (2,2), padding='same')(c3)\n",
        "\n",
        "  # block 4\n",
        "  c4 = Conv2D(128, (4,4), activation='relu')(m3)\n",
        "  f1 = Flatten()(c4)\n",
        "  d1 = Dense(4096, activation='sigmoid')(f1)\n",
        "\n",
        "  return Model(inputs=[inp], outputs=[d1], name='embedding')"
      ],
      "metadata": {
        "id": "ATDs5mQsnfOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = make_embedding()"
      ],
      "metadata": {
        "id": "9fJ8XhOLrXgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding.summary()"
      ],
      "metadata": {
        "id": "0HylDNVbtC0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Build Distance layer"
      ],
      "metadata": {
        "id": "yCUwI_AEtP-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# siamese L1 Distance class\n",
        "class L1Dist(Layer):\n",
        "\n",
        "  def __init_(self, **kwargs):\n",
        "    super().__inti__()\n",
        "\n",
        "  def call(self, input_embedding, validation_embedding):\n",
        "    return tf.math.abs(input_embedding - validation_embedding)"
      ],
      "metadata": {
        "id": "HXZk_2H3tuHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l1 = L1Dist()"
      ],
      "metadata": {
        "id": "6Y9Bdm45uKw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Make Siamese Model"
      ],
      "metadata": {
        "id": "_f_0aauTuLMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_siamese_model():\n",
        "\n",
        "  input_image = Input(name='input_img', shape=(100, 100, 3))\n",
        "\n",
        "  # validation image\n",
        "  validation_image = Input(name='validation_ing', shape=(100, 100, 3))\n",
        "\n",
        "  # combine siamese distance components\n",
        "  siamese_layer = L1Dist()\n",
        "  siamese_layer._name = 'distance'\n",
        "  distances = siamese_layer(embedding(input_image), embedding(validation_image))\n",
        "\n",
        "  # classification layer\n",
        "  classifier = Dense(1, activation='sigmoid')(distances)\n",
        "\n",
        "  return Model(inputs=[input_image, validation_image], outputs=classifier, name='SiameseNetwork')\n"
      ],
      "metadata": {
        "id": "sLIza8TqvbUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siamese_model = make_siamese_model()"
      ],
      "metadata": {
        "id": "ySCLMBKZww0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siamese_model.summary()"
      ],
      "metadata": {
        "id": "iLK9B701wxKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training"
      ],
      "metadata": {
        "id": "1rIUgfTBzRvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "binary_cross_loss = tf.losses.BinaryCrossentropy()\n"
      ],
      "metadata": {
        "id": "nEsKH0bt4KHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt = tf.keras.optimizers.Adam(1e-4)"
      ],
      "metadata": {
        "id": "imv2GGE80Ocs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Establish Checkpoints"
      ],
      "metadata": {
        "id": "s-MBsBW84a0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
        "checkpoint = tf.train.Checkpoint(opt=opt, siamese_model=siamese_model)\n"
      ],
      "metadata": {
        "id": "HkRmi3kB4iKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build train step function"
      ],
      "metadata": {
        "id": "gVqbkJJ-47SK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(batch):\n",
        "\n",
        "  # record all of the operations\n",
        "  with tf.GradientTape() as tape:\n",
        "\n",
        "    X = batch[:2]\n",
        "    # get label\n",
        "    Y = batch[2]\n",
        "\n",
        "    # forward pass\n",
        "    yhat = siamese_model(X, training=True)\n",
        "    # calculate loss\n",
        "    loss = binary_cross_loss(Y, yhat)\n",
        "\n",
        "  print(loss)\n",
        "\n",
        "  # calculate gradients\n",
        "  grad = tape.gradient(loss, siamese_model.trainable_variables)\n",
        "\n",
        "  # calculate updated weights and apply to siamese model\n",
        "  opt.apply_gradients(zip(grad, siamese_model.trainable_variables))\n",
        "\n",
        "  # return loss\n",
        "  return loss\n"
      ],
      "metadata": {
        "id": "Y7vAUsiL4_s1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "build training loop"
      ],
      "metadata": {
        "id": "VBVsN_3i-X2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import metric calculations\n",
        "from tensorflow.keras.metrics import Precision, Recall"
      ],
      "metadata": {
        "id": "LdQib0HP-w3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(data, EPOCHS):\n",
        "  for epoch in range(1, EPOCHS+1):\n",
        "    print('\\n Epoch {}/{}'.format(epoch, EPOCHS))\n",
        "    progbar = tf.keras.utils.Progbar(len(data))\n",
        "\n",
        "    # initializing metric objects\n",
        "    r = Recall()\n",
        "    p = Precision()\n",
        "\n",
        "    # go through each batch\n",
        "    for idx, batch in enumerate(data):\n",
        "\n",
        "      # train step\n",
        "      loss = train_step(batch)\n",
        "      yhat = siamese_model.predict(batch[:2])\n",
        "      r.update_state(batch[2], yhat)\n",
        "      p.update_state(batch[2], yhat)\n",
        "      progbar.update(idx+1)\n",
        "\n",
        "      print(loss.numpy(), r.result().numpy(), p.result().numpy())\n",
        "\n",
        "      # save checkpoints\n",
        "      if epoch % 10 == 0:\n",
        "        checkpoint.save(file_prefix=checkpoint_prefix)\n"
      ],
      "metadata": {
        "id": "CRjuo6Ue-ji9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train the model"
      ],
      "metadata": {
        "id": "S_x8cKH1AreO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 50"
      ],
      "metadata": {
        "id": "c8MJDAUCClfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(train_data, EPOCHS)"
      ],
      "metadata": {
        "id": "bpQ957ACCoVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate Model"
      ],
      "metadata": {
        "id": "MQSL1HejDDhq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "import metrics"
      ],
      "metadata": {
        "id": "AILrNezMD6EU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import metric calculations\n",
        "from tensorflow.keras.metrics import Precision, Recall"
      ],
      "metadata": {
        "id": "tyLt55GwEYgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "make predictions"
      ],
      "metadata": {
        "id": "a4XLZnVMEgOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a batch of test data\n",
        "test_input, test_val, y_true = test_data.as_numpy_iterator().next()\n"
      ],
      "metadata": {
        "id": "1UrVClrJEoa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_hat = siamese_model.predict([test_input, test_val])"
      ],
      "metadata": {
        "id": "HCtYCrb5E14l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# post processing the results\n",
        "[1 if prediction > 0.5 else 0 for prediction in y_hat]"
      ],
      "metadata": {
        "id": "FqRMkfewE7sb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_true"
      ],
      "metadata": {
        "id": "IPM_xx8YFJg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Calculate Metrics"
      ],
      "metadata": {
        "id": "T1-ljl15FM2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a metric object\n",
        "m = Recall()\n",
        "\n",
        "# calculating the recall value\n",
        "m.update_state(y_true, y_hat)\n",
        "\n",
        "# return recall result\n",
        "m.result().numpy()"
      ],
      "metadata": {
        "id": "SRR5sssTFSnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a metric object\n",
        "m = Precision()\n",
        "\n",
        "# calculating the recall value\n",
        "m.update_state(y_true, y_hat)\n",
        "\n",
        "# return recall result\n",
        "m.result().numpy()"
      ],
      "metadata": {
        "id": "ZIFsB52wFgja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r = Recall()\n",
        "p = Precision()\n",
        "\n",
        "for test_input, test_val, y_true in test_data.as_numpy_iterator():\n",
        "  yhat = siamese_model.predict([test_input, test_val])\n",
        "  r.update_state(y_true, yhat)\n",
        "  p.update_state(y_true, yhat)\n",
        "\n",
        "print(r.result().numpy(), p.result().numpy())"
      ],
      "metadata": {
        "id": "O126E9OKFyPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Visualize results"
      ],
      "metadata": {
        "id": "5OLhIgGkHmoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set plot size\n",
        "plt.figure(figsize=(10,8))\n",
        "\n",
        "# set first subplot\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(test_input[4])\n",
        "\n",
        "# set second subplot\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(test_val[4])\n",
        "\n",
        "# renders cleanly\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XwPLJ0DoHsDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Save Model"
      ],
      "metadata": {
        "id": "KAa2qwREIJQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save weights\n",
        "siamese_model.save('siamesemodelv2.h5')"
      ],
      "metadata": {
        "id": "hH0Cnaa6ILVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "L1Dist"
      ],
      "metadata": {
        "id": "f0B6bo6DIRjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siamese_model = tf.keras.models.load_model('siamesemodelv2.h5', \n",
        "                                   custom_objects={'L1Dist':L1Dist, 'BinaryCrossentropy':tf.losses.BinaryCrossentropy})\n"
      ],
      "metadata": {
        "id": "23WAeeG0UhAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make predictions with reloaded model\n",
        "siamese_model.predict([test_input, test_val])"
      ],
      "metadata": {
        "id": "RJwQ9SIoUk_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siamese_model.summary()"
      ],
      "metadata": {
        "id": "HRTBlg-HU0e_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##save model to google drive"
      ],
      "metadata": {
        "id": "hKEpv8_vgWyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/siamesemodelv2.h5 \"/content/drive/MyDrive/Colab Notebooks/\""
      ],
      "metadata": {
        "id": "ip9aWMKshjgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jCWj71BIh03K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}